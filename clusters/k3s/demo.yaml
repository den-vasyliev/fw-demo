---
# Source: helm/charts/redis/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: demo-redis
  namespace: "demo"
  labels:
    app.kubernetes.io/instance: demo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.0
    helm.sh/chart: redis-20.1.3
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: demo
      app.kubernetes.io/name: redis
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections
    - ports:
        - port: 6379
---
# Source: helm/charts/nats/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: demo-nats
  namespace: demo
  labels:
    helm.sh/chart: nats-0.18.0
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: demo
    app.kubernetes.io/version: "2.9.0"
    app.kubernetes.io/managed-by: Helm
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nats
      app.kubernetes.io/instance: demo
---
# Source: helm/charts/redis/templates/master/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: demo-redis-master
  namespace: "demo"
  labels:
    app.kubernetes.io/instance: demo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.0
    helm.sh/chart: redis-20.1.3
    app.kubernetes.io/component: master
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: demo
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: master
---
# Source: helm/charts/api-gateway/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ambassador
  namespace: demo
---
# Source: helm/charts/nats/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: demo-nats
  namespace: demo
  labels:
    helm.sh/chart: nats-0.18.0
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: demo
    app.kubernetes.io/version: "2.9.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: helm/charts/redis/templates/master/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: demo-redis-master
  namespace: "demo"
  labels:
    app.kubernetes.io/instance: demo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.0
    helm.sh/chart: redis-20.1.3
---
# Source: helm/templates/secret.yaml
apiVersion: v1
data:
  license: MTIzNDU=
kind: Secret
metadata:
  name: demo-secret
  namespace: demo
  labels:
    version: v4
---
# Source: helm/charts/nats/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: demo-nats-config
  namespace: demo
  labels:
    helm.sh/chart: nats-0.18.0
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: demo
    app.kubernetes.io/version: "2.9.0"
    app.kubernetes.io/managed-by: Helm
data:
  nats.conf: |
    # NATS Clients Port
    port: 4222

    # PID file shared with configuration reloader.
    pid_file: "/var/run/nats/nats.pid"

    ###############
    #             #
    # Monitoring  #
    #             #
    ###############
    http: 8222
    server_name:$POD_NAME
    lame_duck_grace_period: 10s
    lame_duck_duration: 30s
---
# Source: helm/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: demo-redis-configuration
  namespace: "demo"
  labels:
    app.kubernetes.io/instance: demo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.0
    helm.sh/chart: redis-20.1.3
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: helm/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: demo-redis-health
  namespace: "demo"
  labels:
    app.kubernetes.io/instance: demo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.0
    helm.sh/chart: redis-20.1.3
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: helm/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: demo-redis-scripts
  namespace: "demo"
  labels:
    app.kubernetes.io/instance: demo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.0
    helm.sh/chart: redis-20.1.3
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: helm/charts/api-gateway/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ambassador
  namespace: demo
rules:
- apiGroups: [""]
  resources:
  - services
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["create", "update", "patch", "get", "list", "watch"]
- apiGroups: [""]
  resources:
  - secrets
  verbs: ["get", "list", "watch"]
---
# Source: helm/charts/api-gateway/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ambassador
  namespace: demo
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ambassador
subjects:
- kind: ServiceAccount
  name: ambassador
  namespace: demo
---
# Source: helm/charts/api-gateway/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    service: ambassador
  name: ambassador
  namespace: demo
spec:
  type: NodePort
  ports:
  - name: ambassador-http
    port: 80
    targetPort: 80
  selector:
    service: ambassador
---
# Source: helm/charts/api-gateway/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    service: ambassador-admin
  name: ambassador-admin
  namespace: demo
spec:
  type: ClusterIP
  ports:
  - name: ambassador-admin
    port: 8877
    targetPort: 8877
  selector:
    service: ambassador
---
# Source: helm/charts/mysql/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: demo-mysql
  namespace: demo
  labels:
    app: demo-mysql
spec:
  ports:
  - name: tcp-mysql
    port: 3306
    targetPort: 3306
  selector:
    app: demo-mysql
---
# Source: helm/charts/nats/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: demo-nats
  namespace: demo
  labels:
    helm.sh/chart: nats-0.18.0
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: demo
    app.kubernetes.io/version: "2.9.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: demo
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
  - name: client
    port: 4222
    appProtocol: tcp
  - name: cluster
    port: 6222
    appProtocol: tcp
  - name: monitor
    port: 8222
    appProtocol: http
  - name: metrics
    port: 7777
    appProtocol: http
  - name: leafnodes
    port: 7422
    appProtocol: tcp
  - name: gateways
    port: 7522
    appProtocol: tcp
---
# Source: helm/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: demo-redis-headless
  namespace: "demo"
  labels:
    app.kubernetes.io/instance: demo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.0
    helm.sh/chart: redis-20.1.3
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: demo
    app.kubernetes.io/name: redis
---
# Source: helm/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: demo-redis-master
  namespace: "demo"
  labels:
    app.kubernetes.io/instance: demo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.0
    helm.sh/chart: redis-20.1.3
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: demo
    app.kubernetes.io/name: redis
    app.kubernetes.io/component: master
---
# Source: helm/templates/api-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: demo-api
  namespace: demo
  labels:
    app: api
    version: v4
  annotations:
    
    getambassador.io/config: |
      ---
      apiVersion: ambassador/v1
      kind:  Mapping
      name:  demo-api-api
      prefix: /
      service: demo-api
      
        
      
       
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8080
  selector:
    app: demo-api
---
# Source: helm/templates/ascii-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: demo-ascii
  namespace: demo
  labels:
    app: ascii
    version: v4
  annotations:
   getambassador.io/config: |
      ---
      apiVersion: ambassador/v1
      kind:  Mapping
      name:  demo-ascii
      prefix: /ascii/
      service: demo-ascii
      
        
      
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8080
  selector:
    app: demo-ascii
---
# Source: helm/templates/data-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: demo-data
  namespace: demo
  labels:
    app: data
    version: v4 
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8080
  selector:
    app: demo-data
---
# Source: helm/templates/img-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: demo-img
  namespace: demo
  labels:
    app: img
    version: v4
  annotations:
   getambassador.io/config: |
      ---
      apiVersion: ambassador/v1
      kind:  Mapping
      name:  demo-img
      prefix: /img/
      service: demo-img
      
        
      
spec:      
  ports:
  - name: http
    port: 80
    targetPort: 8080
  selector:
    app: demo-img
---
# Source: helm/charts/api-gateway/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ambassador
  namespace: demo
spec:
  selector:
    matchLabels:
      service: ambassador
  replicas: 1
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
      labels:
        service: ambassador
    spec:
      serviceAccountName: ambassador
      containers:
      - name: ambassador
        image: quay.io/datawire/ambassador:0.51.2
        resources:
          limits:
            cpu: 500m
            memory: 400Mi
          requests:
            cpu: 200m
            memory: 200Mi
        env:
        - name: AMBASSADOR_SINGLE_NAMESPACE
          value: demo
        - name: AMBASSADOR_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace          
        livenessProbe:
          httpGet:
            path: /ambassador/v0/check_alive
            port: 8877
          initialDelaySeconds: 30
          periodSeconds: 3
        readinessProbe:
          httpGet:
            path: /ambassador/v0/check_ready
            port: 8877
          initialDelaySeconds: 30
          periodSeconds: 3
---
# Source: helm/charts/mysql/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-mysql
  namespace: demo
  labels:
    service: demo-mysql
spec:
  selector:
    matchLabels:
      app: demo-mysql
  replicas: 1
  template:
    metadata:
      labels:
        app: demo-mysql
    spec:
      containers:
      - name: mysql
        image: mysql  # or just image: percona
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "true"
        - name: MYSQL_DATABASE
          value: "demo"
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 3306
---
# Source: helm/charts/nats/templates/nats-box.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-nats-box
  namespace: demo
  labels:
    app: demo-nats-box
    chart: nats-0.18.0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo-nats-box
  template:
    metadata:
      labels:
        app: demo-nats-box
    spec:
      volumes:
      containers:
      - name: nats-box
        image: natsio/nats-box:0.13.0
        imagePullPolicy: IfNotPresent
        resources:
          null
        env:
        - name: NATS_URL
          value: demo-nats
        command:
        - "tail"
        - "-f"
        - "/dev/null"
        volumeMounts:
---
# Source: helm/templates/api-deploy.yaml
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: demo-api
  namespace: demo
  labels:
    version: v4
spec:
  selector:
    matchLabels:
      app: demo-api
  replicas: 1
  template:
    metadata:
      labels:
        app: demo-api
        version: v4
    spec:
      containers:
      - name: api
        image: denvasyliev/k8sdiy:build-b1ee187
        command:
          - ./app
          - "-role"
          - "api"
          - "-server"
          - demo-nats
        imagePullPolicy: Always
        env:
          - name: APP_BACKEND
            value: "iscii"
          - name: APP_DATASTORE
            value: "data"
          - name: APP_DB
            value: "root@tcp(db:3306)/demo"
          - name: APP_CACHE
            value: "demo-redis-master"
          - name: APP_LICENSE
            valueFrom:
              secretKeyRef:
                name: demo-secret
                key: license
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 8080
          name: http
        securityContext:
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 3
          periodSeconds: 3
        readinessProbe:
          httpGet:
            path: /readinez
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 3
---
# Source: helm/templates/ascii-deploy.yaml
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: demo-ascii
  namespace: demo
  labels:
    version: v4
spec:
  selector:
    matchLabels:
      app: demo-ascii
  replicas: 1
  template:
    metadata:
      labels:
        app: demo-ascii
        version: v4
    spec:
      containers:
      - name: ascii
        image: denvasyliev/k8sdiy:build-b1ee187
        command:
          - ./app
          - "-role"
          - "ascii"
          - "-server"
          - demo-nats
        imagePullPolicy: Always
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 8080
          name: http
        securityContext:
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 3
          periodSeconds: 3
        readinessProbe:
          httpGet:
            path: /readinez
            port: 8080
          periodSeconds: 3
---
# Source: helm/templates/data-deploy.yaml
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: demo-data
  namespace: demo
  labels:
    version: v4
spec:
  selector:
    matchLabels:
      app: demo-data
  replicas: 1
  template:
    metadata:
      labels:
        app: demo-data
        version: v4
    spec:
      containers:
      - name: data
        image: denvasyliev/k8sdiy:build-b1ee187
        command:
          - ./app
          - "-role"
          - "data"
          - "-server"
          - demo-nats
        env:
          - name: APP_DB
            value: "root@tcp(demo-mysql:3306)/demo"
          - name: APP_CACHE
            value: "demo-redis-master"
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 8080
        name: data
        securityContext:
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 3
          periodSeconds: 3
        readinessProbe:
          httpGet:
            path: /readinez
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 3
---
# Source: helm/templates/img-deploy.yaml
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: demo-img
  namespace: demo
  labels:
    version: v4
spec:
  selector:
    matchLabels:
      app: demo-img
  replicas: 1
  template:
    metadata:
      labels:
        app: demo-img
        version: v4
    spec:
      containers:
      - name: img
        image: denvasyliev/k8sdiy:build-b1ee187
        command:
          - ./app
          - "-role"
          - "img"
          - "-r"
          - "0.2"
          - "-server"
          - demo-nats
        imagePullPolicy: Always
        env:
          - name: APP_DB
            value: "root@tcp(db:3306)/demo"
          - name: APP_CACHE
            value: "demo-redis-master"
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 8080
        securityContext:
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 3
          periodSeconds: 3
        readinessProbe:
          httpGet:
            path: /readinez
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 3
---
# Source: helm/charts/nats/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: demo-nats
  namespace: demo
  labels:
    helm.sh/chart: nats-0.18.0
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: demo
    app.kubernetes.io/version: "2.9.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: nats
      app.kubernetes.io/instance: demo
  replicas: 1
  serviceName: demo-nats

  podManagementPolicy: Parallel

  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "7777"
        prometheus.io/scrape: "true"
        checksum/config: 5371c4bb3cea54ee11988acc80d4b55eafad24cdb0cbb529fd9d07720504b6ed
      labels:
        app.kubernetes.io/name: nats
        app.kubernetes.io/instance: demo
    spec:
      # Common volumes for the containers.
      volumes:
      - name: config-volume
        configMap:
          name: demo-nats-config

      # Local volume shared with the reloader.
      - name: pid
        emptyDir: {}

      #################
      #               #
      #  TLS Volumes  #
      #               #
      #################

      serviceAccountName: demo-nats

      # Required to be able to HUP signal and apply config
      # reload to the server without restarting the pod.
      shareProcessNamespace: true

      #################
      #               #
      #  NATS Server  #
      #               #
      #################
      terminationGracePeriodSeconds: 60
      containers:
      - name: nats
        image: nats:2.9.0-alpine
        imagePullPolicy: IfNotPresent
        resources:
          {}
        ports:
        - containerPort: 4222
          name: client
        - containerPort: 6222
          name: cluster
        - containerPort: 8222
          name: monitor

        command:
        - "nats-server"
        - "--config"
        - "/etc/nats-config/nats.conf"

        # Required to be able to define an environment variable
        # that refers to other environment variables.  This env var
        # is later used as part of the configuration file.
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_NAME
          value: $(POD_NAME)
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CLUSTER_ADVERTISE
          value: $(POD_NAME).demo-nats.$(POD_NAMESPACE).svc.cluster.local
        volumeMounts:
        - name: config-volume
          mountPath: /etc/nats-config
        - name: pid
          mountPath: /var/run/nats
        

        #######################
        #                     #
        # Healthcheck Probes  #
        #                     #
        #######################
        livenessProbe:
          httpGet:
            path: /
            port: 8222
          initialDelaySeconds: 10
          timeoutSeconds: 5
          periodSeconds: 30
          successThreshold: 1
          failureThreshold: 3
        startupProbe:
          httpGet:
            # for NATS server versions >=2.7.1, healthz will be enabled to allow for a grace period
            # in case of JetStream enabled deployments to form quorum and streams to catch up.
            path: /healthz
            port: 8222
          initialDelaySeconds: 10
          timeoutSeconds: 5
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 30

        # Gracefully stop NATS Server on pod deletion or image upgrade.
        #
        lifecycle:
          preStop:
            exec:
              # send the lame duck shutdown signal to trigger a graceful shutdown
              # nats-server will ignore the TERM signal it receives after this
              #
              command:
              - "nats-server"
              - "-sl=ldm=/var/run/nats/nats.pid"

      #################################
      #                               #
      #  NATS Configuration Reloader  #
      #                               #
      #################################
      - name: reloader
        image: natsio/nats-server-config-reloader:0.7.2
        imagePullPolicy: IfNotPresent
        resources:
          null
        command:
        - "nats-server-config-reloader"
        - "-pid"
        - "/var/run/nats/nats.pid"
        - "-config"
        - "/etc/nats-config/nats.conf"
        volumeMounts:
        - name: config-volume
          mountPath: /etc/nats-config
        - name: pid
          mountPath: /var/run/nats
        

      ##############################
      #                            #
      #  NATS Prometheus Exporter  #
      #                            #
      ##############################
      - name: metrics
        image: natsio/prometheus-nats-exporter:0.10.0
        imagePullPolicy: IfNotPresent
        resources:
          {}
        args:
        - -connz
        - -routez
        - -subz
        - -varz
        - -prefix=nats
        - -use_internal_server_id
        - http://localhost:8222/
        ports:
        - containerPort: 7777
          name: metrics

  volumeClaimTemplates:
---
# Source: helm/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: demo-redis-master
  namespace: "demo"
  labels:
    app.kubernetes.io/instance: demo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.4.0
    helm.sh/chart: redis-20.1.3
    app.kubernetes.io/component: master
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: demo
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: master
  serviceName: demo-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: demo
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.4.0
        helm.sh/chart: redis-20.1.3
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 43cdf68c28f3abe25ce017a82f74dbf2437d1900fd69df51a55a3edf6193d141
        checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    spec:
      
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: demo-redis-master
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: demo
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.4.0-debian-12-r4
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: empty-dir
              mountPath: /opt/bitnami/redis/etc/
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: start-scripts
          configMap:
            name: demo-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: demo-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: demo-redis-configuration
        - name: empty-dir
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: demo
          app.kubernetes.io/name: redis
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: helm/charts/api-gateway/templates/rbac.yaml
# Source: ambassador/templates/rbac.yaml
---
# Source: helm/charts/nats/templates/tests/test-request-reply.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "demo-nats-test-request-reply"
  labels:
    chart: nats-0.18.0
    app: demo-nats-test-request-reply
  annotations:
    "helm.sh/hook": test
spec:
  containers:
  - name: nats-box
    image: synadia/nats-box
    env:
    - name: NATS_HOST
      value: demo-nats
    command:
    - /bin/sh
    - -ec
    - |
      nats reply -s nats://$NATS_HOST:4222 'name.>' --command "echo 1" &
    - |
      "&&"
    - |
      name=$(nats request -s nats://$NATS_HOST:4222 name.test '' 2>/dev/null)
    - |
      "&&"
    - |
      [ $name = test ]

  restartPolicy: Never
